{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lqg1d\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import collect_episodes\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import collect_episodes, estimate_performance\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = lqg1d.LQG1D(initial_state_type='random')\n",
    "discount = 0.9\n",
    "horizon = 50\n",
    "\n",
    "actions = discrete_actions = np.linspace(-8, 8, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Show the optimal Q-function\n",
    "#################################################################\n",
    "def make_grid(x, y):\n",
    "    m = np.meshgrid(x, y, copy=False, indexing='ij')\n",
    "    return np.vstack(m).reshape(2, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = discrete_states = np.linspace(-10, 10, 20)\n",
    "SA = make_grid(states, actions)\n",
    "S, A = SA[:, 0], SA[:, 1]\n",
    "\n",
    "K, cov = env.computeOptimalK(discount), 0.001\n",
    "print('Optimal K: {} Covariance S: {}'.format(K, cov))\n",
    "\n",
    "Q_fun_ = np.vectorize(lambda s, a: env.computeQFunction(s, a, K, cov, discount, 1))\n",
    "Q_fun = lambda X: Q_fun_(X[:, 0], X[:, 1])\n",
    "\n",
    "Q_opt = Q_fun(SA)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(S, A, Q_opt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehPolicy:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        \n",
    "    def draw_action(self, state):\n",
    "        return self.actions[np.random.randint(len(self.actions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FQIPolicy:\n",
    "    def __init__(self, Q, actions):\n",
    "        self.Q = Q\n",
    "        self.actions = actions\n",
    "    \n",
    "    def draw_action(self, state):\n",
    "        # return self.actions[(np.abs(self.actions-Q(state, -(theta[0]+state*theta[1])/2*theta[2]))).argmin()]\n",
    "        return self.actions[np.argmax([Q(state, a) for a in self.actions])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Collect the samples using the behavioural policy\n",
    "#################################################################\n",
    "# You should use discrete actions\n",
    "theta = np.zeros((3,))\n",
    "phi = lambda s,a: np.array([a, s*a, s**2+a**2])\n",
    "Q = lambda s,a: phi(float(s),float(a)).dot(theta.T)\n",
    "lmbda = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define FQI\n",
    "# to evaluate the policy you can use estimate_performance\n",
    "fqi = FQIPolicy(Q, actions)\n",
    "beh_policy = BehPolicy(actions)\n",
    "\n",
    "n_itr = 100\n",
    "\n",
    "dataset = collect_episodes(env, n_episodes=100, policy=beh_policy, horizon=horizon)\n",
    "#np.argmax([Q(state, a) for a in self.actions]) \n",
    "\n",
    "Z = np.array([phi(dataset[t][\"states\"][i],dataset[t][\"actions\"][i]).transpose()  for t in range(len(dataset)) for i in range(len(dataset[t][\"actions\"]))])\n",
    "# y = np.array([dataset[t][\"rewards\"][i] + discount*Q(dataset[t][\"next_states\"][i], -(theta[0]+dataset[t][\"next_states\"][i]*theta[1])/2*theta[2]) for t in range(len(dataset))  for i in range(len(dataset[t][\"actions\"]))])\n",
    "y = np.array([dataset[t][\"rewards\"][i] + discount*np.max([Q(dataset[t][\"next_states\"][i], a) for a in beh_policy.actions])  for t in range(len(dataset))  for i in range(len(dataset[t][\"actions\"]))])\n",
    "\n",
    "theta = np.linalg.inv(Z.transpose().dot(Z)+lmbda*np.eye(Z.shape[1])).dot(Z.transpose()).dot(y)\n",
    "J_t = []\n",
    "for _ in tqdm(range(n_itr), desc=\"Simulating\"):\n",
    "    #dataset = collect_episodes(env, n_episodes=100,\n",
    "    #                                            policy=fqi, horizon=horizon)\n",
    "    #np.argmax([Q(state, a) for a in self.actions]) \n",
    "\n",
    "    Z = np.array([phi(episode[\"states\"][i], episode[\"actions\"][i]).transpose()  for episode in dataset for i in range(len(episode[\"actions\"]))])\n",
    "    # y = np.array([dataset[t][\"rewards\"][i] + discount*Q(dataset[t][\"next_states\"][i], -(theta[0]+dataset[t][\"next_states\"][i]*theta[1])/2*theta[2]) for t in range(len(dataset))  for i in range(len(dataset[t][\"actions\"]))])\n",
    "    y = np.array([episode[\"rewards\"][i] + discount*np.max([Q(episode[\"next_states\"][i], a) for a in fqi.actions])  for episode in dataset  for i in range(len(episode[\"actions\"]))])\n",
    "\n",
    "    theta = np.linalg.inv(Z.T.dot(Z)+lmbda*np.eye(Z.shape[1])).dot(Z.T).dot(y)\n",
    "    J_t.append(estimate_performance(env, policy=fqi, horizon=10, n_episodes=50, gamma=discount))\n",
    "    print(theta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(J_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot obtained Q-function against the true one\n",
    "J = estimate_performance(env, policy=fqi, horizon=10, n_episodes=50, gamma=discount)\n",
    "print('Policy performance: {}'.format(J))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
