{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lqg1d\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantStep(object):\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, gt):\n",
    "        return self.learning_rate * gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedConstantStep(object) :  \n",
    "    def __init__(self, learning_rate) :\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def update(self, g) :\n",
    "        delta_g = self.learning_rate/np.linalg.norm(g) * g\n",
    "        return delta_g   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamStep(object) :\n",
    "    \n",
    "    def __init__(self, learning_rate, beta1 = 0.9, beta2 = 0.999, epsilon = 10**(-8)) :\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.beta1_power = beta1\n",
    "        self.beta2_power = beta2\n",
    "        \n",
    "    def update(self, g) :\n",
    "        self.m = self.beta1*self.m + (1-self.beta1)*g\n",
    "        self.v = self.beta2*self.v + (1-self.beta2)*np.multiply(g,g)\n",
    "        m_hat = self.m / (1 - self.beta1_power)\n",
    "        v_hat = self.v / (1 - self.beta2_power)\n",
    "        delta_g = self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        self.beta1_power *= self.beta1\n",
    "        self.beta2_power *= self.beta2\n",
    "        return delta_g   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy:\n",
    "    def __init__(self, theta, sigma):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def draw_action(self, s):\n",
    "        return np.random.normal(s*self.theta, self.sigma)\n",
    "    \n",
    "    def gradient_log(self, a, s):\n",
    "        d_theta = (a-s*self.theta)/(self.sigma**2)*s\n",
    "        d_sigma = ((a-s*self.theta)**2 - self.sigma**2)/(self.sigma**3)\n",
    "        return np.array([d_theta, d_sigma])\n",
    "    \n",
    "    def update(self, theta, sigma):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(paths, policy, discount):\n",
    "    return np.mean([\n",
    "        np.sum([\n",
    "            policy.gradient_log(path['actions'][t], path['states'][t]) for t in range(len(path['states']))],0)*\n",
    "             np.sum([\n",
    "                path['rewards'][r]*discount**r for r in range(len(path['states']))\n",
    "            ])\n",
    "        for path in paths\n",
    "    ],0)\n",
    "    return np.mean([\n",
    "        np.sum( [\n",
    "            policy.gradient_log(paths[n]['actions'][t],paths[n]['states'][t])[0]\n",
    "            for t in range(0,len(paths[n]['states']))])* \n",
    "            np.sum([\n",
    "                paths[n]['rewards'][r]*discount**r for r in range(len(paths[n]['states']))\n",
    "            ])\n",
    "        for n in range(0,N)]), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Define the environment and the policy\n",
    "#####################################################\n",
    "env = lqg1d.LQG1D(initial_state_type='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Experiments parameters\n",
    "#####################################################\n",
    "# We will collect N trajectories per iteration\n",
    "N = 1\n",
    "# Each trajectory will have at most T time steps\n",
    "T = 100\n",
    "# Number of policy parameters updates\n",
    "n_itr = 50000\n",
    "# Set the discount factor for the problem\n",
    "discount = 0.9\n",
    "# Learning rate for the gradient update\n",
    "learning_rate = 0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = GaussianPolicy(-0.1, 0.5)\n",
    "\n",
    "#####################################################\n",
    "# define the update rule (stepper)\n",
    "stepper =  ConstantStep(learning_rate) # e.g., constant, adam or anything you want\n",
    "adam_stepper = AdamStep() # e.g., constant, adam or anything you want\n",
    "normalized_stepper = NormalizedConstantStep(learning_rate)\n",
    "\n",
    "\n",
    "# fill the following part of the code with\n",
    "#  - REINFORCE estimate i.e. gradient estimate\n",
    "#  - update of policy parameters using the steppers\n",
    "#  - average performance per iteration\n",
    "#  - distance between optimal mean parameter and the one at it k\n",
    "mean_parameters = []\n",
    "avg_return = []\n",
    "all_theta = [policy.theta]\n",
    "for i in tqdm(range(n_itr), desc=\"Simulating\"):\n",
    "    paths = utils.collect_episodes(env, policy=policy, horizon=T, n_episodes=N)\n",
    "    d_theta, d_sigma = estimate_gradient(paths, policy, discount)\n",
    "    # sigma remains constant\n",
    "    policy.update(policy.theta+adam_stepper.update(d_theta), policy.sigma)\n",
    "    all_theta.append(policy.theta)\n",
    "    avg_return.append(np.mean([np.sum([\n",
    "                paths[n]['rewards'][r]*(discount**r) for r in range(len(paths[n]['states']))\n",
    "            ]) for n in range(0,N)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(all_theta)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(all_theta)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average return obtained by simulating the policy\n",
    "# at each iteration of the algorithm (this is a rought estimate\n",
    "# of the performance\n",
    "plt.figure()\n",
    "plt.plot(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distance mean parameter\n",
    "# of iteration k\n",
    "plt.figure()\n",
    "plt.plot(mean_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
