{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lqg1d\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantStep(object):\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, gt):\n",
    "        return self.learning_rate * gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamStep(object) :\n",
    "    \n",
    "    def __init__(self, learning_rate) :\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def update(self, g, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, alpha = 0.01) :\n",
    "        self.m = beta1*self.m + (1-beta1)*g\n",
    "        self.v = beta2*self.v + (1-beta2)*np.multiply(g,g)\n",
    "        m_hat = self.m / (1 - beta1)\n",
    "        v_hat = self.v / (1 - beta1)\n",
    "        delta_g = alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "        return delta_g   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy:\n",
    "    def __init__(self, theta, sigma):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def draw_action(self, s):\n",
    "        return np.random.normal(s*self.theta, self.sigma)\n",
    "    \n",
    "    def gradient_log(self, a, s):\n",
    "        d_theta = (a-s*self.theta)/(self.sigma**2)*s\n",
    "        #print(\"gradient_log\")\n",
    "        #print(a, s)\n",
    "         #print(self.theta)\n",
    "        #print((a-s*self.theta))\n",
    "        #print((a-s*self.theta)/(self.sigma**2)*s)\n",
    "        d_sigma = ((a-s*self.theta)**2 - self.sigma**2)/(self.sigma**3)\n",
    "        return np.array([d_theta, d_sigma])\n",
    "    \n",
    "    def update(self, theta, sigma):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(paths, policy, discount):\n",
    "    \"\"\"\n",
    "    return np.mean(\n",
    "        [np.sum(\n",
    "            [path[\"rewards\"][i]*(discount**i) for i in range(len(path[\"rewards\"]))]\n",
    "            ) * np.sum(\n",
    "            [policy.gradient_log(path[\"actions\"][i], path[\"states\"][i]) for i in range(len(path[\"actions\"]))]\n",
    "            , 0) for path in paths\n",
    "        ]\n",
    "        , 0)\n",
    "    \"\"\"\n",
    "    return np.mean([np.sum( [policy.gradient_log(paths[n]['actions'][t],paths[n]['states'][t])[0] * np.sum([paths[n]['rewards'][r]*(discount**r) for r in range(len(paths[n]['states']))])  for t in range(0,len(paths[n]['states']))]) for n in range(0,N)]), 0\n",
    "\n",
    "    \"\"\"\n",
    "    d_theta = 0\n",
    "    d_sigma = 0\n",
    "    for path in paths:\n",
    "        R = 0\n",
    "        for i in range(1,1+len(path[\"rewards\"])):\n",
    "            R = discount*R + path[\"rewards\"][-i]\n",
    "        for i in range(len(path[\"states\"])):\n",
    "            d_t, d_s = R*policy.gradient_log(path[\"actions\"][i], path[\"states\"][i])\n",
    "            #print(policy.gradient_log(path[\"actions\"][i], path[\"states\"][i]))\n",
    "            #print(path[\"actions\"][i], path[\"states\"][i])\n",
    "            d_theta += d_t\n",
    "            d_sigma += d_s\n",
    "            \n",
    "    return d_theta/len(paths), d_sigma/(len(paths))\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Define the environment and the policy\n",
    "#####################################################\n",
    "env = lqg1d.LQG1D(initial_state_type='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Experiments parameters\n",
    "#####################################################\n",
    "# We will collect N trajectories per iteration\n",
    "N = 100\n",
    "# Each trajectory will have at most T time steps\n",
    "T = 100\n",
    "# Number of policy parameters updates\n",
    "n_itr = 100\n",
    "# Set the discount factor for the problem\n",
    "discount = 0.9\n",
    "# Learning rate for the gradient update\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = GaussianPolicy(-1, 0.5)\n",
    "\n",
    "#####################################################\n",
    "# define the update rule (stepper)\n",
    "stepper =  ConstantStep(learning_rate) # e.g., constant, adam or anything you want\n",
    "adam_stepper = AdamStep(learning_rate) # e.g., constant, adam or anything you want\n",
    "\n",
    "\n",
    "# fill the following part of the code with\n",
    "#  - REINFORCE estimate i.e. gradient estimate\n",
    "#  - update of policy parameters using the steppers\n",
    "#  - average performance per iteration\n",
    "#  - distance between optimal mean parameter and the one at it k\n",
    "mean_parameters = []\n",
    "avg_return = []\n",
    "all_theta = []\n",
    "for _ in tqdm(range(n_itr), desc=\"Simulating\"):\n",
    "    paths = utils.collect_episodes(env, policy=policy, horizon=T, n_episodes=N)\n",
    "    d_theta, d_sigma = estimate_gradient(paths, policy, discount)\n",
    "    print(policy.theta, d_theta, d_sigma)\n",
    "    # sigma remains constant\n",
    "    policy.update(policy.theta+adam_stepper.update(d_theta), policy.sigma)\n",
    "    all_theta.append(policy.theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(all_theta)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy.theta, policy.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = utils.collect_episodes(env, policy=policy, horizon=T, n_episodes=N)\n",
    "d_theta, d_sigma = estimate_gradient(paths, policy, discount)\n",
    "\n",
    "print(d_theta, d_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.update(policy.theta+stepper.update(d_theta), policy.sigma)\n",
    "print(policy.theta, policy.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepper.update(d_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_best = GaussianPolicy(-0.69, 0.5)\n",
    "paths = utils.collect_episodes(env, policy=policy_best, horizon=T, n_episodes=N)\n",
    "d_theta, d_sigma = estimate_gradient(paths, policy_best, discount)\n",
    "print(d_theta, d_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = utils.collect_episodes(env, policy=policy, horizon=T, n_episodes=N)\n",
    "paths_n = utils.collect_episodes(env, policy=GaussianPolicy(-0.69,0.5), horizon=T, n_episodes=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([sum([path[\"rewards\"][i]*discount**i for i in range(len(path[\"rewards\"]))]) for path in paths_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average return obtained by simulating the policy\n",
    "# at each iteration of the algorithm (this is a rought estimate\n",
    "# of the performance\n",
    "plt.figure()\n",
    "plt.plot(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distance mean parameter\n",
    "# of iteration k\n",
    "plt.figure()\n",
    "plt.plot(mean_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = GaussianPolicy(1, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_gradient(paths, GaussianPolicy(1, 0.5), discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.draw_action(-498.98726007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = utils.collect_episodes(env, policy=policy, horizon=T, n_episodes=N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[0][\"states\"][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-500*(-499.91451651 + 500)/(0.5**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.gradient_log(-499.91, -500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.gradient_log(-500.90692724,-500.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
